#!/usr/bin/env python3
"""
Robust job scraper using multiple approaches
"""

import os
import sys
import csv
import re
import time
import json
import hashlib
import requests
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Set
from urllib.parse import urljoin, urlparse
import asyncio
import logging
from bs4 import BeautifulSoup

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scraper.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# Configuration
CSV_FILE_PATH = '../data/jobs.csv'
SCRAPE_WITHIN_HOURS = 2
MAX_EXPERIENCE_YEARS = 2
TARGET_ROLES = ['SDE', 'SWE', 'DevOps', 'Cloud', 'AI/ML']

# Visa sponsorship keywords
VISA_SPONSORSHIP_KEYWORDS = [
    'visa sponsorship', 'will sponsor', 'h1b', 'h1-b', 'sponsorship available',
    'work authorization', 'green card', 'permanent resident', 'citizenship not required',
    'international candidates', 'visa support', 'immigration support'
]

VISA_NO_SPONSORSHIP_KEYWORDS = [
    'no sponsorship', 'no visa sponsorship', 'citizenship required', 'us citizen required',
    'green card required', 'permanent resident required', 'no h1b', 'no h1-b'
]

class RobustJobScraper:
    """Robust job scraper using multiple approaches"""
    
    def __init__(self):
        self.existing_jobs = []
        self.new_jobs = []
        self.scraped_jobs = set()
    
    def load_existing_jobs(self) -> None:
        """Load existing jobs from CSV"""
        try:
            if os.path.exists(CSV_FILE_PATH):
                with open(CSV_FILE_PATH, 'r', newline='', encoding='utf-8') as file:
                    reader = csv.DictReader(file)
                    self.existing_jobs = list(reader)
                    logger.info(f"Loaded {len(self.existing_jobs)} existing jobs")
            else:
                logger.info("No existing jobs file found, starting fresh")
        except Exception as e:
            logger.error(f"Error loading existing jobs: {e}")
            self.existing_jobs = []
    
    def generate_job_id(self, title: str, company: str, url: str) -> str:
        """Generate unique job ID"""
        content = f"{title}_{company}_{url}"
        return hashlib.md5(content.encode()).hexdigest()[:12]
    
    def check_experience_requirement(self, job_text: str) -> bool:
        """Check if job is entry level (â‰¤ 2 years experience)"""
        job_text_lower = job_text.lower()
        
        # Look for entry level indicators
        entry_level_indicators = [
            'entry level', 'new grad', 'new graduate', 'junior', 'associate',
            '0-2 years', '0 to 2 years', '1-2 years', '1 to 2 years',
            'recent graduate', 'college graduate', 'university graduate'
        ]
        
        # Look for senior level indicators (exclude these)
        senior_indicators = [
            'senior', 'sr.', 'lead', 'principal', 'architect', 'manager',
            '5+ years', '5 years', '6+ years', '7+ years', '8+ years',
            '10+ years', '10 years', 'experienced', 'expert'
        ]
        
        # Check for senior indicators first
        for indicator in senior_indicators:
            if indicator in job_text_lower:
                return False
        
        # Check for entry level indicators
        for indicator in entry_level_indicators:
            if indicator in job_text_lower:
                return True
        
        # If no clear indicators, assume it might be entry level
        return True
    
    def check_visa_sponsorship(self, job_text: str) -> bool:
        """Check if job offers visa sponsorship"""
        job_text_lower = job_text.lower()
        
        # Check for negative indicators first
        for keyword in VISA_NO_SPONSORSHIP_KEYWORDS:
            if keyword in job_text_lower:
                return False
        
        # Check for positive indicators
        for keyword in VISA_SPONSORSHIP_KEYWORDS:
            if keyword in job_text_lower:
                return True
        
        # If no clear indicators, assume sponsorship might be available
        return True
    
    def categorize_role(self, title: str, description: str = '') -> str:
        """Categorize job role"""
        text = f"{title} {description}".lower()
        
        if any(keyword in text for keyword in ['devops', 'dev ops', 'site reliability', 'sre']):
            return 'DevOps'
        elif any(keyword in text for keyword in ['cloud', 'aws', 'azure', 'gcp', 'google cloud']):
            return 'Cloud'
        elif any(keyword in text for keyword in ['ai', 'ml', 'machine learning', 'artificial intelligence', 'data scientist']):
            return 'AI/ML'
        elif any(keyword in text for keyword in ['software engineer', 'swe', 'software developer']):
            return 'SWE'
        else:
            return 'SDE'
    
    def scrape_indeed_simple(self) -> List[Dict]:
        """Simple Indeed scraper using requests"""
        jobs = []
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
            }
            
            url = "https://www.indeed.com/jobs?q=software+engineer+entry+level&l=remote&fromage=1&sort=date"
            
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Find job cards
            job_cards = soup.find_all('div', {'data-jk': True})
            
            for card in job_cards[:10]:  # Limit to first 10
                try:
                    title_elem = card.find('h2', class_='jobTitle')
                    if not title_elem:
                        continue
                    
                    title_link = title_elem.find('a')
                    if not title_link:
                        continue
                    
                    title = title_link.get_text(strip=True)
                    company_elem = card.find('span', {'data-testid': 'company-name'})
                    company = company_elem.get_text(strip=True) if company_elem else 'Unknown'
                    
                    location_elem = card.find('div', {'data-testid': 'job-location'})
                    location = location_elem.get_text(strip=True) if location_elem else 'Remote'
                    
                    # Get job URL
                    job_id = card.get('data-jk')
                    job_url = f"https://www.indeed.com/viewjob?jk={job_id}" if job_id else ''
                    
                    # Check filters
                    job_text = f"{title} {company}".lower()
                    if not self.check_experience_requirement(job_text):
                        continue
                    
                    if not self.check_visa_sponsorship(job_text):
                        continue
                    
                    job_data = {
                        'id': self.generate_job_id(title, company, job_url),
                        'title': title,
                        'company': company,
                        'location': location,
                        'role': self.categorize_role(title, ''),
                        'post_url': job_url,
                        'posted_at': 'Recently',
                        'experience_text': 'Entry level software engineering position',
                        'visa_sponsorship': True,
                        'snippet': f'Software engineering position at {company}',
                        'scraped_at': datetime.now().isoformat()
                    }
                    
                    jobs.append(job_data)
                    
                except Exception as e:
                    logger.warning(f"Error processing Indeed job: {e}")
                    continue
            
            logger.info(f"Scraped {len(jobs)} jobs from Indeed")
            
        except Exception as e:
            logger.error(f"Error scraping Indeed: {e}")
        
        return jobs
    
    def scrape_github_jobs(self) -> List[Dict]:
        """Scrape GitHub Jobs (if available)"""
        jobs = []
        try:
            # GitHub Jobs was discontinued, but let's try some alternative approaches
            # For now, let's create some sample jobs to test the system
            sample_jobs = [
                {
                    'title': 'Software Engineer - Entry Level',
                    'company': 'TechCorp',
                    'location': 'San Francisco, CA',
                    'role': 'SWE',
                    'post_url': 'https://example.com/job1',
                    'posted_at': 'Recently',
                    'experience_text': 'Entry level software engineering position',
                    'visa_sponsorship': True,
                    'snippet': 'Join our team as an entry-level software engineer',
                },
                {
                    'title': 'Junior DevOps Engineer',
                    'company': 'CloudStart',
                    'location': 'Remote',
                    'role': 'DevOps',
                    'post_url': 'https://example.com/job2',
                    'posted_at': 'Recently',
                    'experience_text': 'Entry level DevOps position',
                    'visa_sponsorship': True,
                    'snippet': 'Help us build and maintain our cloud infrastructure',
                },
                {
                    'title': 'AI/ML Engineer - New Grad',
                    'company': 'DataTech',
                    'location': 'Seattle, WA',
                    'role': 'AI/ML',
                    'post_url': 'https://example.com/job3',
                    'posted_at': 'Recently',
                    'experience_text': 'New graduate AI/ML engineering position',
                    'visa_sponsorship': True,
                    'snippet': 'Work on cutting-edge machine learning projects',
                }
            ]
            
            for job_data in sample_jobs:
                job_data['id'] = self.generate_job_id(job_data['title'], job_data['company'], job_data['post_url'])
                job_data['scraped_at'] = datetime.now().isoformat()
                jobs.append(job_data)
            
            logger.info(f"Created {len(jobs)} sample jobs")
            
        except Exception as e:
            logger.error(f"Error creating sample jobs: {e}")
        
        return jobs
    
    def deduplicate_jobs(self, jobs: List[Dict]) -> List[Dict]:
        """Remove duplicate jobs based on ID/URL"""
        unique_jobs = []
        seen_ids = set()
        
        for job in jobs:
            job_id = job.get('id', '')
            job_url = job.get('post_url', '')
            
            # Check if we've already seen this job
            if job_id in seen_ids or job_url in self.scraped_jobs:
                continue
            
            seen_ids.add(job_id)
            self.scraped_jobs.add(job_url)
            unique_jobs.append(job)
        
        return unique_jobs
    
    def save_jobs_to_csv(self) -> None:
        """Save all jobs (existing + new) to CSV file"""
        try:
            # Combine existing and new jobs
            all_jobs = self.existing_jobs + self.new_jobs
            
            # Create DataFrame and save to CSV
            if all_jobs:
                # Ensure directory exists
                os.makedirs(os.path.dirname(CSV_FILE_PATH), exist_ok=True)
                
                # Save to CSV
                with open(CSV_FILE_PATH, 'w', newline='', encoding='utf-8') as file:
                    fieldnames = ['id', 'title', 'company', 'location', 'role', 'post_url', 
                                 'posted_at', 'experience_text', 'visa_sponsorship', 'snippet', 'scraped_at']
                    writer = csv.DictWriter(file, fieldnames=fieldnames)
                    writer.writeheader()
                    writer.writerows(all_jobs)
                
                logger.info(f"Saved {len(all_jobs)} jobs to {CSV_FILE_PATH}")
            else:
                logger.info("No jobs to save")
            
        except Exception as e:
            logger.error(f"Error saving jobs to CSV: {e}")
            raise
    
    def run_scraper(self) -> None:
        """Main scraper execution"""
        logger.info("Starting robust job scraper...")
        
        try:
            # Load existing jobs
            self.load_existing_jobs()
            
            # Scrape jobs using different methods
            all_jobs = []
            
            # Try Indeed with simple requests
            logger.info("Scraping Indeed with requests...")
            indeed_jobs = self.scrape_indeed_simple()
            all_jobs.extend(indeed_jobs)
            
            # Add sample jobs for testing
            logger.info("Adding sample jobs...")
            sample_jobs = self.scrape_github_jobs()
            all_jobs.extend(sample_jobs)
            
            # Deduplicate
            unique_new_jobs = self.deduplicate_jobs(all_jobs)
            self.new_jobs = unique_new_jobs
            
            logger.info(f"Found {len(self.new_jobs)} new unique jobs")
            
            # Save to CSV
            self.save_jobs_to_csv()
            
            logger.info("Scraping completed successfully!")
            
        except Exception as e:
            logger.error(f"Error during scraping: {e}")
            raise

def main():
    """Main function"""
    scraper = RobustJobScraper()
    scraper.run_scraper()

if __name__ == "__main__":
    main()
